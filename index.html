<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="RaR">
  <meta property="og:title" content="Reinforcement Learning from Human Feedback with Active Queries"/>
  <meta property="og:description" content="We propose an active learning based direct preference optimization method"/>
  <meta property="og:url" content="https://uclaml.github.io/PDE/"/>
  

  <meta name="twitter:title" content="Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves">
  <meta name="twitter:description" content="We propose Rephrase and Respond (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM, prompting, RaR, Rephrase and Respond">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Reinforcement Learning from Human Feedback with Active Queries</title>
  <link rel="icon" type="image/x-icon" href="static/images/star.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>  
    <SCRIPT SRC='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></SCRIPT>
    <SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}})</SCRIPT>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Reinforcement Learning from Human Feedback with Active Queries</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://sites.google.com/g.ucla.edu/yihedeng/home" target="_blank">Yihe Deng</a>,</span>
                <span class="author-block">
                  <a href="https://web.cs.ucla.edu/~weightzero/" target="_blank">Weitong Zhang</a>,</span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/zxchen/" target="_blank">Zixiang Chen</a>,</span>
                    <span class="author-block">
                      <a href="https://web.cs.ucla.edu/~qgu/" target="_blank">Quanquan Gu</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of California, Los Angeles</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2311.04205.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jkx19/ActiveQuery/tree/web" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.04205" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization APPO algorithm with a constant regret bound and a constant query complexity. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- APPO. -->
<div class="columns is-centered">
    <div class="column is-three-fifths">
      <h2 class="title is-3">Active Proximal Policy Optimization</h2>
  
      <img src="images/algo.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The details of our algorithm Active Proximal Policy Optimization
        </h2>
      <h3 class="title is-4">(One-step) RaR</h3>
      <div class="content has-text-justified">
        <p>
          We propose Active Proximal Policy Optimization (APPO) for learning linear contextual bandits with global sub-optimal gap. In each round, the algorithm first use the following MLE estimator to estimate the parameter:
          $$ \lambda\kappa_{\sigma} \bm{theta} + \sum_{\tau \in \mathcal{C}_{t-1}}\Big(o_{\tau} - \mu\big(\langle \bm{theta}, \bm{phi}^1_{\tau}-\bm{phi}^2_{\tau} \rangle\big)\Big) (\bm{phi}^1_{\tau}-\bm{phi}^2_{\tau}) = \bm{0} $$.
          With the estimated parameter, APPO then compute the estimated reward and choose the best arm. APPO will not query for the label if the uncertainty is low. Our theoretical analysis shows the following guarantees regarding regret upper bound and query complexity.
        </p>
        <img src="images/them.png" alt="MY ALT TEXT"/>
      </div>
      <br/>
    </div>
</div>
<!--/ APPO. -->

<!-- ADPO. -->
<div class="columns is-centered">
    <div class="column is-three-fifths">
      <h2 class="title is-3">Active Direct Preference Optimization</h2>

      <h3 class="title is-4">Method</h3>
      <div class="content has-text-justified">
        <p>
          We further proposed Active Direct Preference Optimization (ADPO) as a label-efficient alternative to Direct Preference Optimization. For each piece of prompt and the two answers, DPO treat the LLM as a reward model and assign rewards to the answers. Therefore, the difference of the two scores indicates the model's confidence of its prediction of the label. In ADPO, LLM will only queries the laebls of those answer pairs with high uncertainty. For those with low uncertainty, LLM will use its predicted label as its training target.
        </p>
        <!-- <img src="images/them.png" alt="MY ALT TEXT"/> -->
      </div>
      <br/>
      <!--/ RaR. -->
  
      <!-- Two-step RaR -->
      <h3 class="title is-4">Experiment Results</h3>
      <div class="content has-text-justified">
        <p>
          We trained zephry-7b-sft-full on the 62k Ultrachat-Feedback dataset using both DPO and our method ADPO. We evaluate the trained models on Open LLM LeaderBoard dataset. We also tested the performance of our method but without training on the pseudo-labels (denoted as ADPO w/o PL). Here are the key points of our results:
        </p>
        <ul>
            <li>while only quering about half (32k in 62k) of human preference labels than DPO, ADPO outperforms DPO on Arc Challenge, TruthfulQA, HellaSwag dataset and get a higher average score than DPO; </li>
            <li>without the pseudo-labels predicted by the LLM, the performance of ADPO w/o PL can no longer out perform DPO, so the pseudo-labels play a key role in ADPO; </li>
          </ul>
        <img src="images/them.png" alt="MY ALT TEXT"/>
      </div>
      <br/>

      <img src="images/table.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The results of DPO and ADPO. ADPO outperforms DPO with only half queries
        </h2>

      <img src="images/chart.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The training curves of DPO, ADPO w/o PL and ADPO on different datasets. ADPO enjoys a faster improvement than DPO and ADPO w/o PL.
        </h2>
  
    </div>
</div>
<!--/ ADPO. -->


<!-- Results. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-4">Performance across Various LLMs</h2>
      <div class="content has-text-justified">
        <p>
          We further examine the performance of RaR on various LLMs, including GPT-3.5 and Vicuna. 
          In particular, we employ Two-step RaR to investigate (1) if all these LLMs can provide consistent 
          response improvement by rephrasing the questions; and (2) if the GPT-4-rephrased questions can improve 
          the performance of other LLMs.
        </p>
        <ul>
          <li>All models can benefit from rephrasing questions, with more advanced models expected to gain a larger improvement.</li>
          <li>The rephrased questions are transferable: the questions rephrased by GPT-4 can improve the response quality on Vicuna.</li>
        </ul>
    </div>
  </div>
</div>
<!--/ Results. -->
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="images/models.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Accuracy (%) of GPT-4-0613, GPT-3.5-turbo-0613 and Vicuna-13b when testing on original and self-rephrased questions using Two-step RaR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="images/models_exp.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Examples of the self-rephrased questions generated by different LLM models.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="images/models_table.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Comparison of GPT-4's rephrased questions with Vicuna's self-rephrased questions.
        </h2>
      </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{deng2023rephrase,
        title={Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves}, 
        author={Yihe Deng and Weitong Zhang and Zixiang Chen and Quanquan Gu},
        year={2023},
        eprint={2311.04205},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>